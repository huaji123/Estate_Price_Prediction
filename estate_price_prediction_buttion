import warnings

import paddle
import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

warnings.filterwarnings("ignore")

datafile = './housing_data.txt'

#
housing_data = np.loadtxt(datafile, dtype=float)
# 特征名字
feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT',
                 'MEDV']
# 特征数量
feature_num = len(feature_names)

print(housing_data)
print(housing_data.shape[0])
# 将数据集换成
# housing_data = housing_data.reshape([housing_data.shape[0], feature_num])

# 画图看特征间的关系，主要是变量两两之间的关系
features_np = np.array([x[:13] for x in housing_data], np.float32)
labels_np = np.array([x[-1] for x in housing_data], np.float32)
# pd.DataFrame创建一个表格，有列名和行名，默认用数字标明
df = pd.DataFrame(housing_data, columns=feature_names)
print(df)
matplotlib.use('TkAgg')
# 展现变量两两之间的关系，线性、非线性、相关等
sns.pairplot(df.dropna(), y_vars=feature_names[-1], x_vars=feature_names[::-1], diag_kind='kde')
plt.show()

# 相关性分析
fig, ax = plt.subplots(figsize=(15, 1))
corr_data = df.corr().iloc[-1]
# 将corr_data重塑为1行14列的矩阵
corr_data = np.asarray(corr_data).reshape(1, 14)
ax = sns.heatmap(corr_data, cbar=True, annot=True)
plt.show()

# 数据归一化处理
sns.boxplot(data=df.iloc[:, 0:13])
plt.show()

features_max = housing_data.max(axis=0)
features_min = housing_data.min(axis=0)
features_avg = housing_data.sum(axis=0) / housing_data.shape[0]

# batch_size = 20 一次喂入神经网络的数据为20组
batch_size = 20


# 参数归一化函数
def feature_norm(input):
    f_size = input.shape
    output_features = np.zeros(f_size, np.float32)
    for batch_id in range(f_size[0]):
        for index in range(13):
            output_features[batch_id][index] = (input[batch_id][index] - features_avg[index]) / (
                        features_max[index] - features_min[index])
    return output_features


# 只对属性进行归一化, 将前13列传入归一化函数
housing_features = feature_norm(housing_data[:, :13])
# 将归一化的数据与标签进行合并；np.c_函数就是让矩阵按列合并
housing_data = np.c_[housing_features, housing_data[:, -1].astype(np.float32)]

print(housing_data)

# 归一化后的train_data，看下个属性的情况
features_np = np.array([x[:13] for x in housing_data], np.float32)
labels_np = np.array([x[-1] for x in housing_data], np.float32)
data_np = np.c_[features_np, labels_np]
df2 = pd.DataFrame(data_np, columns=feature_names)
re_data = df2.iloc[:, 0:13]

sns.boxplot(data=df2.iloc[:, 0:13])
plt.show()

# 将训练数据集和测试数据集按照8：2的比例分开
ratio = 0.8
offset = int(housing_data.shape[0] * ratio)
train_data = housing_data[:offset]
test_data = housing_data[offset:]


class Regressor(paddle.nn.Layer):
    def __init__(self):
        super(Regressor, self).__init__()
        self.fc = paddle.nn.Linear(13, 1,)

    def forward(self, inputs):
        pred = self.fc(inputs)
        return pred


# 定制绘制训练过程的损失值变化趋势的方法 draw_train_process
train_nums = []
train_costs = []


def draw_train_process(iters, train_costs):
    plt.title("training cost", fontsize=24)
    plt.xlabel("iter", fontsize=14)
    plt.ylabel("cost", fontsize=14)
    plt.plot(iters, train_costs, color='red', label='training cost')
    plt.show()


# 使用梯度下降发对损失函数进行优化
import paddle.nn.functional as F

y_preds = []
labels_list = []


def train(model):
    print('start training...')
    model.train()
    epoch_num = 500
    train_num = 0
    optimizer = paddle.optimizer.SGD(learning_rate=0.001, parameters=model.parameters())
    for epoch_id in range(epoch_num):
        # 在每轮迭代开始之前，将训练数据的顺序随机打乱
        np.random.shuffle(train_data)
        # 每次喂入一个batch_size大小的数据
        mini_batches = [train_data[k: k + batch_size] for k in range(0, len(train_data), batch_size)]
        for batch_id, data in enumerate(mini_batches):
            features_np = np.array(data[:, :13], np.float32)
            labels_np = np.array(data[:,-1:], np.float32)
            features = paddle.to_tensor(features_np)
            labels = paddle.to_tensor(labels_np)
            # 前向计算
            y_pred = model(features)
            cost = F.mse_loss(y_pred, label=labels)
            train_cost = cost.numpy()[0]
            # 反向传播
            cost.backward()
            # 最小化loss，更新参数
            optimizer.step()
            # 消除梯度
            optimizer.clear_grad()

            if batch_id % 30 == 0 and epoch_id % 50 == 0:
                print("Pass:%d,Cost:%0.5f" %(epoch_id, train_cost))

            train_num = train_num + batch_size
            train_nums.append(train_num)
            train_costs.append(train_cost)


model = Regressor()
train(model)

matplotlib.use('TkAgg')
draw_train_process(train_nums, train_costs)

# 获取预测数据
INFER_BATCH_SIZE = 100

infer_features_np = np.array([data[:13] for data in test_data]).astype("float32")
infer_labels_np = np.array([data[-1] for data in test_data]).astype("float32")

infer_features = paddle.to_tensor(infer_features_np)
infer_labels = paddle.to_tensor(infer_labels_np)
fetch_list = model(infer_features)

sum_cost = 0
for i in range(INFER_BATCH_SIZE):
    infer_result = fetch_list[i][0]
    ground_truth = infer_labels[i]
    if i % 10 == 0:
        print("No.%d: infer result is %.2f,ground truth is %.2f" % (i, infer_result, ground_truth))
    cost = paddle.pow(infer_result - ground_truth, 2)
    sum_cost += cost
mean_loss = sum_cost / INFER_BATCH_SIZE
print("Mean loss is:", mean_loss.numpy())

def plot_pred_ground(pred, ground):
    plt.figure()
    plt.title("Predication v.s. Ground truth", fontsize=24)
    plt.xlabel("ground truth price(unit:$1000)", fontsize=14)
    plt.ylabel("predict price", fontsize=14)
    plt.scatter(ground, pred, alpha=0.5)  #  scatter:散点图,alpha:"透明度"
    plt.plot(ground, ground, c='red')
    plt.show()

plot_pred_ground(fetch_list, infer_labels_np)
